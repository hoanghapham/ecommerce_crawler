{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vipon_crawler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vipon_crawler.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Replace account's username & password here\n",
    "username = os.environ['vipon_username']\n",
    "password = os.environ['vipon_password']\n",
    "\n",
    "class ViponCrawler:\n",
    "    \n",
    "    def __init__(self, username, password):\n",
    "        \n",
    "        print('Initating...', end='\\n')\n",
    "        display = Display(size=(800,600), visible=False)\n",
    "        display.start()\n",
    "        \n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.wait = WebDriverWait(self.driver, 20)\n",
    "        self.deals_url = 'https://www.vipon.com/promotion/index?type=instant'\n",
    "        self.code_base_url = 'https://www.vipon.com/code/get-code?id={}'\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        \n",
    "    def login(self):\n",
    "        \n",
    "        print('Logging in...', end='\\n')\n",
    "        self.driver.get('https://www.vipon.com/login?ref=menu_login_mobile')\n",
    "\n",
    "        form_email = self.driver.find_element_by_css_selector('input[id=\"loginform-email\"]')\n",
    "        form_email.send_keys(self.username)\n",
    "        form_password = self.driver.find_element_by_css_selector('input[id=\"loginform-password\"]')\n",
    "        form_password.send_keys(self.password)\n",
    "        submit_button = self.driver.find_element_by_css_selector('button[type=\"submit\"]')\n",
    "        submit_button.click()\n",
    "        \n",
    "    def get_link_count(self):\n",
    "        deals_soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "        elm_count = len(deals_soup.select('div .layer'))\n",
    "        return elm_count\n",
    "    \n",
    "    def get_links(self, max_link_count=200):\n",
    "        \n",
    "        print('Getting links...', end='\\n')\n",
    "        self.driver.get(self.deals_url)\n",
    "        self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div .layer')), 'Timeout.')\n",
    "\n",
    "        last_link_count = 0\n",
    "\n",
    "        while last_link_count < max_link_count:\n",
    "\n",
    "            self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "\n",
    "            try:\n",
    "                self.wait.until(lambda driver: last_link_count < self.get_link_count(), \"Timeout\")\n",
    "            except TimeoutException:\n",
    "                print('Cannot get more links')\n",
    "                pass\n",
    "\n",
    "            last_link_count = self.get_link_count()\n",
    "    \n",
    "        deals_soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "        prod_elms = deals_soup.select('div .layer')\n",
    "        prod_urls = [x.get('onclick').split('\\'')[1] for x in prod_elms]\n",
    "        print('Links get: {}'.format(len(prod_urls)))\n",
    "        \n",
    "        self.prod_urls = prod_urls\n",
    "        \n",
    "    def parse_info(self, get_code=False):\n",
    "\n",
    "        self.data_dict = {\n",
    "            'product_id': []\n",
    "            , 'title': []\n",
    "            , 'category': []\n",
    "            , 'like': []\n",
    "            , 'dislike': []\n",
    "            , 'discount': []\n",
    "            , 'code': []\n",
    "            , 'expiry_time': []\n",
    "            , 'list_price': []\n",
    "            , 'sales_price': []\n",
    "            , 'amazon_url': []\n",
    "        }\n",
    "\n",
    "        for counter, url in enumerate(self.prod_urls):\n",
    "            \n",
    "            print('Checking link {} of {}'.format(counter + 1, len(self.prod_urls)), end = '\\r')\n",
    "            \n",
    "            self.driver.get(url)\n",
    "            prod_soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "\n",
    "            # Basic information\n",
    "            product_id = pd.to_numeric(url.split('/')[-1])\n",
    "\n",
    "            title = prod_soup.select('p[class=product-title]')[0] \\\n",
    "                                .getText(strip=True, separator = ',')\n",
    "\n",
    "            category, dislike, like = prod_soup.select('div .product-category')[0] \\\n",
    "                                            .getText(strip=True, separator=';').split(';')\n",
    "\n",
    "            discount = prod_soup.select('p[class=product-discount]')[0] \\\n",
    "                                .getText(strip=True, separator = ',')\n",
    "\n",
    "            expiry_time = prod_soup.select('span[id=productExpiry]')[0] \\\n",
    "                                    .select('b')[0].getText()\n",
    "\n",
    "            list_price, sales_price = prod_soup.select('p[class=product-price]')[0] \\\n",
    "                                                .getText(strip=True, separator = ',').split(',')\n",
    "\n",
    "            amazon_url = prod_soup.select('a[onclick=\"bing_open_in_amazon();\"]')[0] \\\n",
    "                                    .get('href')\n",
    "            \n",
    "            self.data_dict['product_id'].append(product_id)\n",
    "            self.data_dict['title'].append(title)\n",
    "            self.data_dict['category'].append(category)\n",
    "            self.data_dict['like'].append(pd.to_numeric(like))\n",
    "            self.data_dict['dislike'].append(pd.to_numeric(dislike))\n",
    "            self.data_dict['discount'].append(discount)\n",
    "            self.data_dict['expiry_time'].append(expiry_time)\n",
    "            self.data_dict['list_price'].append(list_price)\n",
    "            self.data_dict['sales_price'].append(sales_price)\n",
    "            self.data_dict['amazon_url'].append(amazon_url)\n",
    "            \n",
    "            time.sleep(random.randint(2, 10))\n",
    "            \n",
    "            # Get code\n",
    "            if get_code == True:\n",
    "                self.login()\n",
    "                code_url = self.code_base_url.format(product_id)\n",
    "                self.driver.get(code_url)\n",
    "                code_soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "                try:\n",
    "                    code = code_soup.select('div .code-container')[0] \\\n",
    "                            .getText(separator=',', strip=True) \\\n",
    "                            .split(',')[-1]\n",
    "                except IndexError:\n",
    "                    code = np.nan\n",
    "            else:\n",
    "                code = np.nan\n",
    "                \n",
    "            self.data_dict['code'].append(code)\n",
    "            \n",
    "            time.sleep(random.randint(2, 10))\n",
    "            \n",
    "        self.deals_info = pd.DataFrame(self.data_dict)\n",
    "        self.deals_info['crawl_date'] = datetime.date.today()\n",
    "        print('Finished.', end='\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler = ViponCrawler(username, password)\n",
    "    crawler.get_links(max_link_count=20)\n",
    "    crawler.parse_info(get_code=False)\n",
    "\n",
    "    crawler.deals_info.to_csv(\n",
    "        'deals_info_{}.csv'.format(datetime.date.strftime(datetime.date.today(), '%Y%m%d'))\n",
    "        , index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
